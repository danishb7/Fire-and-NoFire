{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e5590f-845e-406a-8bc8-76ff0375b5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all files from 'Training.zip' into ''.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_files(zip_path):\n",
    "    extract_dir = os.path.dirname(zip_path)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Extracted all files from '{zip_path}' into '{extract_dir}'.\")\n",
    "\n",
    "extract_files(\"Training.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32334cc-e059-448c-8510-c97f6dc3cc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7642ae-3cc6-4d93-a374-5ae0d8cd8d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbhatka/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dbhatka/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 39375\n",
      "Training images: 29531\n",
      "Validation images: 9844\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"Training\"\n",
    "\n",
    "# image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resizing for ResNet input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ResNet normalization\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# splits training data into 3:1 ratio\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# dataloaders for training\n",
    "# batch_size = 32\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_features, 2)\n",
    "\n",
    "# for checking purposes\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49bd416-d318-4c5f-b1bc-7940d6b8fa11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|██████████| 1846/1846 [04:05<00:00,  7.52it/s]\n",
      "Validating: 100%|██████████| 616/616 [01:05<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.0950, Train Accuracy: 0.9687, Val Loss: 0.5843, Val Accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 1846/1846 [02:03<00:00, 14.96it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 0.0358, Train Accuracy: 0.9876, Val Loss: 0.0270, Val Accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 1846/1846 [02:03<00:00, 14.99it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:31<00:00, 19.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Train Loss: 0.0217, Train Accuracy: 0.9927, Val Loss: 0.0170, Val Accuracy: 0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 1846/1846 [02:06<00:00, 14.57it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Train Loss: 0.0170, Train Accuracy: 0.9943, Val Loss: 0.0059, Val Accuracy: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 1846/1846 [02:03<00:00, 14.90it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:31<00:00, 19.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 0.0141, Train Accuracy: 0.9956, Val Loss: 0.0095, Val Accuracy: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 1846/1846 [02:00<00:00, 15.26it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Train Loss: 0.0106, Train Accuracy: 0.9968, Val Loss: 0.0051, Val Accuracy: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 1846/1846 [01:59<00:00, 15.42it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:29<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Train Loss: 0.0106, Train Accuracy: 0.9966, Val Loss: 0.0099, Val Accuracy: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 1846/1846 [01:59<00:00, 15.43it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:31<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Train Loss: 0.0088, Train Accuracy: 0.9973, Val Loss: 0.0031, Val Accuracy: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 1846/1846 [01:58<00:00, 15.51it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Train Loss: 0.0059, Train Accuracy: 0.9980, Val Loss: 0.0070, Val Accuracy: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 1846/1846 [01:58<00:00, 15.64it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:29<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Train Loss: 0.0098, Train Accuracy: 0.9972, Val Loss: 0.0050, Val Accuracy: 0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 1846/1846 [01:59<00:00, 15.47it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Train Loss: 0.0049, Train Accuracy: 0.9988, Val Loss: 0.0034, Val Accuracy: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 1846/1846 [02:00<00:00, 15.32it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:32<00:00, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Train Loss: 0.0059, Train Accuracy: 0.9981, Val Loss: 0.0022, Val Accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 1846/1846 [02:03<00:00, 14.89it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Train Loss: 0.0036, Train Accuracy: 0.9986, Val Loss: 0.0025, Val Accuracy: 0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 1846/1846 [02:00<00:00, 15.32it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:29<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Train Loss: 0.0051, Train Accuracy: 0.9984, Val Loss: 0.0058, Val Accuracy: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 1846/1846 [01:57<00:00, 15.74it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:29<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Train Loss: 0.0033, Train Accuracy: 0.9988, Val Loss: 0.0039, Val Accuracy: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 1846/1846 [01:59<00:00, 15.48it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:29<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Train Loss: 0.0038, Train Accuracy: 0.9988, Val Loss: 0.0024, Val Accuracy: 0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 1846/1846 [01:58<00:00, 15.61it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:29<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Train Loss: 0.0040, Train Accuracy: 0.9988, Val Loss: 0.0081, Val Accuracy: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 1846/1846 [01:58<00:00, 15.62it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 20.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Train Loss: 0.0047, Train Accuracy: 0.9986, Val Loss: 0.0023, Val Accuracy: 0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 1846/1846 [02:00<00:00, 15.38it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Train Loss: 0.0031, Train Accuracy: 0.9992, Val Loss: 0.0056, Val Accuracy: 0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 1846/1846 [02:01<00:00, 15.24it/s]\n",
      "Validating: 100%|██████████| 616/616 [00:30<00:00, 19.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Train Loss: 0.0030, Train Accuracy: 0.9987, Val Loss: 0.0030, Val Accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sets device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# moves the model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# training/validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / total_train\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / total_val\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38840148-2da7-4cd1-abdc-891af0456c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to flame_classification.pth\n"
     ]
    }
   ],
   "source": [
    "# for saving the model\n",
    "model_save_path = \"flame_classification.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157e37b7-f06a-4775-b32f-9486be4f9687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.1058835/ipykernel_1267829/2754892897.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for loading the model\n",
    "model_load_path = \"flame_classification.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aee1ca0-eb14-4b07-8951-860341f7de1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 270/270 [00:50<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# test dataset transformations\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "test_dir = \"Test\"\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# model testing\n",
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
